{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhanushbabuk/Skill-Lab/blob/main/Gen_AI_Lab_Programs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL624YiACjGG",
        "outputId": "bb08d3c7-ce71-4f50-b8bc-2cfcc432ffaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['on', 'log', 'played', 'cat', 'dog', 'mat', 'and', 'the', 'sat', 'together']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.         0.         0.08109302\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.08109302 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.08109302 0.         0.         0.\n",
            "  0.08109302 0.         0.         0.08109302]]\n"
          ]
        }
      ],
      "source": [
        "# Question 1: Computing the TF-IDF Matrix using NumPy\n",
        "# Task: Write a Python function to compute the TF-IDF matrix for the given set of documents using only NumPy.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def compute_tf_idf(documents, vocabulary):\n",
        "    N = len(documents)\n",
        "    V = len(vocabulary)\n",
        "\n",
        "    # Initialize TF matrix (N x V)\n",
        "    tf = np.zeros((N, V))\n",
        "\n",
        "    # Build term frequency matrix\n",
        "    for i, doc in enumerate(documents):\n",
        "        words = doc.lower().split()\n",
        "        for word in words:\n",
        "            if word in vocabulary:\n",
        "                j = vocabulary.index(word)\n",
        "                tf[i, j] += 1\n",
        "        tf[i] = tf[i] / len(words)  # Normalize TF by document length\n",
        "\n",
        "    # Compute Document Frequency (DF)\n",
        "    df = np.zeros(V)\n",
        "    for j, term in enumerate(vocabulary):\n",
        "        df[j] = sum(1 for doc in documents if term in doc.lower().split())\n",
        "\n",
        "    # Compute Inverse Document Frequency (IDF)\n",
        "    idf = np.log(N / (df + 1))  # Add 1 to avoid division by zero\n",
        "\n",
        "    # Compute TF-IDF matrix\n",
        "    tf_idf = tf * idf  # Element-wise multiplication\n",
        "\n",
        "    return tf_idf\n",
        "\n",
        "# Example usage:\n",
        "documents = [\n",
        "    \"cat sat on the mat\",\n",
        "    \"dog sat on the log\",\n",
        "    \"cat and dog played together\"\n",
        "]\n",
        "\n",
        "vocabulary = ['on', 'log', 'played', 'cat', 'dog', 'mat', 'and', 'the', 'sat', 'together']\n",
        "\n",
        "tf_idf_matrix = compute_tf_idf(documents, vocabulary)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"TF-IDF Matrix:\\n\", tf_idf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2: Generating n-grams for a Sentence\n",
        "# Task: Write a Python function to generate n-grams for a given sentence.\n",
        "\n",
        "\n",
        "def generate_ngrams(sentence, n):\n",
        "    words = sentence.lower().split()\n",
        "    ngrams = []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        ngram = tuple(words[i:i + n])\n",
        "        ngrams.append(ngram)\n",
        "    return ngrams\n",
        "\n",
        "# Example usage:\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "n = 3\n",
        "ngrams = generate_ngrams(sentence, n)\n",
        "print(f\"{n}-grams:\")\n",
        "for gram in ngrams:\n",
        "    print(gram)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO8u7aejCrgQ",
        "outputId": "814874b1-d401-404d-a137-8d7b51c71d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3-grams:\n",
            "('the', 'quick', 'brown')\n",
            "('quick', 'brown', 'fox')\n",
            "('brown', 'fox', 'jumps')\n",
            "('fox', 'jumps', 'over')\n",
            "('jumps', 'over', 'the')\n",
            "('over', 'the', 'lazy')\n",
            "('the', 'lazy', 'dog.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3: Computing a 3-gram Language Model\n",
        "# Task: Write a Python function to compute a 3-gram language model.\n",
        "\n",
        "def compute_trigram_language_model(documents):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    trigram_counts = defaultdict(int)\n",
        "    total_trigrams = 0\n",
        "\n",
        "    for doc in documents:\n",
        "        words = doc.lower().split()\n",
        "        for i in range(len(words) - 2):\n",
        "            trigram = tuple(words[i:i + 3])\n",
        "            trigram_counts[trigram] += 1\n",
        "            total_trigrams += 1\n",
        "\n",
        "    # Compute probabilities\n",
        "    trigram_probabilities = {}\n",
        "    for trigram, count in trigram_counts.items():\n",
        "        trigram_probabilities[trigram] = count / total_trigrams\n",
        "\n",
        "    return trigram_probabilities\n",
        "\n",
        "# Example usage:\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The quick blue fox jumps over the lazy cat\",\n",
        "    \"The lazy dog sleeps under the blue sky\"\n",
        "]\n",
        "\n",
        "trigram_model = compute_trigram_language_model(documents)\n",
        "\n",
        "print(\"Trigram Probabilities:\")\n",
        "for trigram, prob in trigram_model.items():\n",
        "    print(f\"{trigram}: {prob}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrJ2mE6eCrcq",
        "outputId": "ae4527e0-3598-4fcd-d518-20cdd1e10788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram Probabilities:\n",
            "('the', 'quick', 'brown'): 0.05\n",
            "('quick', 'brown', 'fox'): 0.05\n",
            "('brown', 'fox', 'jumps'): 0.05\n",
            "('fox', 'jumps', 'over'): 0.1\n",
            "('jumps', 'over', 'the'): 0.1\n",
            "('over', 'the', 'lazy'): 0.1\n",
            "('the', 'lazy', 'dog'): 0.1\n",
            "('the', 'quick', 'blue'): 0.05\n",
            "('quick', 'blue', 'fox'): 0.05\n",
            "('blue', 'fox', 'jumps'): 0.05\n",
            "('the', 'lazy', 'cat'): 0.05\n",
            "('lazy', 'dog', 'sleeps'): 0.05\n",
            "('dog', 'sleeps', 'under'): 0.05\n",
            "('sleeps', 'under', 'the'): 0.05\n",
            "('under', 'the', 'blue'): 0.05\n",
            "('the', 'blue', 'sky'): 0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 4: Creating a Word Embedding Matrix\n",
        "# Task:\n",
        "# 1. Implement the function create_embedding_matrix(corpus, embedding_dim).\n",
        "# 2. Test the function and get_word_vector with the given corpus and embedding_dim=3.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(corpus, embedding_dim):\n",
        "    # Preprocessing\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in corpus:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    V = len(vocabulary)\n",
        "    # Initialize embedding matrix with random values between 0 and 1\n",
        "    E = np.random.rand(V, embedding_dim)\n",
        "\n",
        "    # Create word to index mapping (already done in vocabulary)\n",
        "    word_to_index = vocabulary\n",
        "\n",
        "    # Define get_word_vector function\n",
        "    def get_word_vector(word):\n",
        "        word = word.lower()\n",
        "        if word in word_to_index:\n",
        "            idx = word_to_index[word]\n",
        "            return E[idx]\n",
        "        else:\n",
        "            return np.zeros(embedding_dim)\n",
        "\n",
        "    return E, vocabulary, get_word_vector\n",
        "\n",
        "# Example usage:\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "embedding_dim = 3\n",
        "\n",
        "E, vocabulary, get_word_vector = create_embedding_matrix(corpus, embedding_dim)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"Embedding Matrix E:\\n\", E)\n",
        "\n",
        "# Test get_word_vector\n",
        "word = \"learning\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)\n",
        "\n",
        "# Test with a word not in the vocabulary\n",
        "word = \"unknown\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plyHcfN4CraJ",
        "outputId": "676ba7e6-3a86-4c30-fcb2-6bfda8b1dd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "Embedding Matrix E:\n",
            " [[0.02662826 0.915984   0.77666595]\n",
            " [0.25376714 0.21514975 0.4448874 ]\n",
            " [0.47606384 0.66183713 0.08300569]\n",
            " [0.20259067 0.70095195 0.7575685 ]\n",
            " [0.04519184 0.56372675 0.9600809 ]\n",
            " [0.10284511 0.96287873 0.48486379]\n",
            " [0.11178872 0.15055291 0.40998136]\n",
            " [0.26888766 0.28651446 0.24958493]]\n",
            "Embedding for 'learning': [0.20259067 0.70095195 0.7575685 ]\n",
            "Embedding for 'unknown': [0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 5: Creating a Word Embedding Matrix with Pre-trained Embeddings\n",
        "# Task:\n",
        "# 1. Implement the function create_embedding_matrix_with_pretrained(corpus, pretrained_embeddings, embedding_dim).\n",
        "# 2. Test the function with the given corpus and pre-trained embeddings.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix_with_pretrained(corpus, pretrained_embeddings, embedding_dim):\n",
        "    # Preprocessing\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in corpus:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    V = len(vocabulary)\n",
        "    # Initialize embedding matrix\n",
        "    E = np.zeros((V, embedding_dim))\n",
        "\n",
        "    # Assign embeddings\n",
        "    for word, idx in vocabulary.items():\n",
        "        if word in pretrained_embeddings:\n",
        "            E[idx] = np.array(pretrained_embeddings[word])\n",
        "        else:\n",
        "            E[idx] = np.random.rand(embedding_dim)  # Random initialization\n",
        "\n",
        "    # Define get_word_vector function\n",
        "    def get_word_vector(word):\n",
        "        word = word.lower()\n",
        "        if word in vocabulary:\n",
        "            idx = vocabulary[word]\n",
        "            return E[idx]\n",
        "        else:\n",
        "            return np.zeros(embedding_dim)\n",
        "\n",
        "    return E, vocabulary, get_word_vector\n",
        "\n",
        "# Example usage:\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "\n",
        "pretrained_embeddings = {\n",
        "    \"machine\": [0.1, 0.2, 0.3],\n",
        "    \"learning\": [0.2, 0.3, 0.4],\n",
        "    \"amazing\": [0.3, 0.4, 0.5],\n",
        "    \"love\": [0.4, 0.5, 0.6]\n",
        "}\n",
        "\n",
        "embedding_dim = 3\n",
        "\n",
        "E, vocabulary, get_word_vector = create_embedding_matrix_with_pretrained(corpus, pretrained_embeddings, embedding_dim)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"Embedding Matrix E:\\n\", E)\n",
        "\n",
        "# Test get_word_vector\n",
        "word = \"machine\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)\n",
        "\n",
        "word = \"i\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)  # Randomly initialized\n",
        "\n",
        "word = \"unknown\"\n",
        "vector = get_word_vector(word)\n",
        "print(f\"Embedding for '{word}':\", vector)  # Returns zeros\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgcJZDpMCrXH",
        "outputId": "5ecd5081-5868-4289-a7a8-ee03bd685873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "Embedding Matrix E:\n",
            " [[0.55680945 0.29140246 0.55984242]\n",
            " [0.4        0.5        0.6       ]\n",
            " [0.1        0.2        0.3       ]\n",
            " [0.2        0.3        0.4       ]\n",
            " [0.86748338 0.46406756 0.00387744]\n",
            " [0.3        0.4        0.5       ]\n",
            " [0.41404741 0.55373404 0.52458677]\n",
            " [0.27329535 0.45573975 0.84564351]]\n",
            "Embedding for 'machine': [0.1 0.2 0.3]\n",
            "Embedding for 'i': [0.55680945 0.29140246 0.55984242]\n",
            "Embedding for 'unknown': [0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Generating One-Hot Encodings\n",
        "# Task:\n",
        "# 1. Implement the function create_one_hot_encodings(corpus).\n",
        "# 2. Test the function with the given corpus.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def create_one_hot_encodings(corpus):\n",
        "    # Preprocessing\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in corpus:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    V = len(vocabulary)\n",
        "    # Initialize one-hot encoding matrix\n",
        "    one_hot_encodings = {}\n",
        "\n",
        "    for word, idx in vocabulary.items():\n",
        "        one_hot_vector = np.zeros(V)\n",
        "        one_hot_vector[idx] = 1\n",
        "        one_hot_encodings[word] = one_hot_vector\n",
        "\n",
        "    return vocabulary, one_hot_encodings\n",
        "\n",
        "# Example usage:\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "\n",
        "vocabulary, one_hot_encodings = create_one_hot_encodings(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"\\nOne-Hot Encodings:\")\n",
        "for word, one_hot_vector in one_hot_encodings.items():\n",
        "    print(f\"Word: '{word}' - One-Hot Vector: {one_hot_vector}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFO4sZ6vCrUU",
        "outputId": "2ed58a83-013d-4649-f1f2-b89288dee906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "\n",
            "One-Hot Encodings:\n",
            "Word: 'i' - One-Hot Vector: [1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Word: 'love' - One-Hot Vector: [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Word: 'machine' - One-Hot Vector: [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Word: 'learning' - One-Hot Vector: [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Word: 'is' - One-Hot Vector: [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "Word: 'amazing' - One-Hot Vector: [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Word: 'new' - One-Hot Vector: [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Word: 'things' - One-Hot Vector: [0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Implementing the Skip-Gram Model\n",
        "# Task:\n",
        "# 1. Implement the function generate_skip_gram_pairs(sentences, window_size).\n",
        "# 2. Test it with the given sentences and window_size = 2.\n",
        "\n",
        "\n",
        "def generate_skip_gram_pairs(sentences, window_size):\n",
        "    # Preprocessing: Build the vocabulary and word indices\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    # Generate skip-gram training pairs\n",
        "    training_pairs = []\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        for i, target_word in enumerate(words):\n",
        "            # Define the context window\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(words), i + window_size + 1)\n",
        "            for j in range(start, end):\n",
        "                if i != j:\n",
        "                    context_word = words[j]\n",
        "                    training_pairs.append((target_word, context_word))\n",
        "\n",
        "    return vocabulary, training_pairs\n",
        "\n",
        "# Example usage:\n",
        "sentences = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "\n",
        "window_size = 2\n",
        "\n",
        "vocabulary, training_pairs = generate_skip_gram_pairs(sentences, window_size)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"\\nSkip-Gram Training Pairs:\")\n",
        "for pair in training_pairs:\n",
        "    print(pair)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1D4NUxkD-es",
        "outputId": "8ba560b6-da5b-4e67-de98-d95adac4cd8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "\n",
            "Skip-Gram Training Pairs:\n",
            "('i', 'love')\n",
            "('i', 'machine')\n",
            "('love', 'i')\n",
            "('love', 'machine')\n",
            "('love', 'learning')\n",
            "('machine', 'i')\n",
            "('machine', 'love')\n",
            "('machine', 'learning')\n",
            "('learning', 'love')\n",
            "('learning', 'machine')\n",
            "('machine', 'learning')\n",
            "('machine', 'is')\n",
            "('learning', 'machine')\n",
            "('learning', 'is')\n",
            "('learning', 'amazing')\n",
            "('is', 'machine')\n",
            "('is', 'learning')\n",
            "('is', 'amazing')\n",
            "('amazing', 'learning')\n",
            "('amazing', 'is')\n",
            "('i', 'love')\n",
            "('i', 'learning')\n",
            "('love', 'i')\n",
            "('love', 'learning')\n",
            "('love', 'new')\n",
            "('learning', 'i')\n",
            "('learning', 'love')\n",
            "('learning', 'new')\n",
            "('learning', 'things')\n",
            "('new', 'love')\n",
            "('new', 'learning')\n",
            "('new', 'things')\n",
            "('things', 'learning')\n",
            "('things', 'new')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Generating CBOW Training Pairs\n",
        "# Task:\n",
        "# 1. Implement the function generate_cbow_pairs(sentences, window_size).\n",
        "# 2. Test it with the given sentences and window_size = 2.\n",
        "\n",
        "def generate_cbow_pairs(sentences, window_size):\n",
        "    # Preprocessing: Build the vocabulary and word indices\n",
        "    vocabulary = {}\n",
        "    index = 0\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        for word in words:\n",
        "            if word not in vocabulary:\n",
        "                vocabulary[word] = index\n",
        "                index += 1\n",
        "\n",
        "    # Generate CBOW training pairs\n",
        "    training_pairs = []\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        for i, target_word in enumerate(words):\n",
        "            # Define the context window\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(words), i + window_size + 1)\n",
        "            context_words = []\n",
        "            for j in range(start, end):\n",
        "                if i != j:\n",
        "                    context_words.append(words[j])\n",
        "            if context_words:\n",
        "                training_pairs.append((tuple(context_words), target_word))\n",
        "\n",
        "    return vocabulary, training_pairs\n",
        "\n",
        "# Example usage:\n",
        "sentences = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love learning new things\"\n",
        "]\n",
        "\n",
        "window_size = 2\n",
        "\n",
        "vocabulary, training_pairs = generate_cbow_pairs(sentences, window_size)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"\\nCBOW Training Pairs:\")\n",
        "for pair in training_pairs:\n",
        "    print(f\"Context: {pair[0]}, Target: {pair[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2AskN5zD-bF",
        "outputId": "beb5ee5d-8373-44b9-9610-5e32c273c0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'i': 0, 'love': 1, 'machine': 2, 'learning': 3, 'is': 4, 'amazing': 5, 'new': 6, 'things': 7}\n",
            "\n",
            "CBOW Training Pairs:\n",
            "Context: ('love', 'machine'), Target: i\n",
            "Context: ('i', 'machine', 'learning'), Target: love\n",
            "Context: ('i', 'love', 'learning'), Target: machine\n",
            "Context: ('love', 'machine'), Target: learning\n",
            "Context: ('learning', 'is'), Target: machine\n",
            "Context: ('machine', 'is', 'amazing'), Target: learning\n",
            "Context: ('machine', 'learning', 'amazing'), Target: is\n",
            "Context: ('learning', 'is'), Target: amazing\n",
            "Context: ('love', 'learning'), Target: i\n",
            "Context: ('i', 'learning', 'new'), Target: love\n",
            "Context: ('i', 'love', 'new', 'things'), Target: learning\n",
            "Context: ('love', 'learning', 'things'), Target: new\n",
            "Context: ('learning', 'new'), Target: things\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Implementing a Simple Vanilla RNN\n",
        "# Task:\n",
        "# 1. Implement the function rnn_forward(x, Wxh, Whh, Why, bh, by, h0).\n",
        "# 2. Test the function with random weights, biases, and an initial hidden state.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def rnn_forward(x, Wxh, Whh, Why, bh, by, h0):\n",
        "    h = h0\n",
        "    hs = []\n",
        "    ys = []\n",
        "    for t in range(len(x)):\n",
        "        xt = np.array([[x[t]]])  # Input at time t (make it a column vector)\n",
        "        h = np.tanh(np.dot(Whh, h) + np.dot(Wxh, xt) + bh)  # Hidden state\n",
        "        y = np.dot(Why, h) + by  # Output\n",
        "        hs.append(h)\n",
        "        ys.append(y)\n",
        "    return ys, hs\n",
        "\n",
        "# Example usage:\n",
        "# Input sequence\n",
        "x = [1, 2, 3]\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 1   # Since x is a sequence of numbers\n",
        "hidden_size = 4  # You can choose any size for hidden state\n",
        "output_size = 1  # Output is a single number at each time step\n",
        "\n",
        "# Random initialization of weights and biases\n",
        "np.random.seed(0)  # For reproducibility\n",
        "Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "Why = np.random.randn(output_size, hidden_size) * 0.01\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))\n",
        "h0 = np.zeros((hidden_size, 1))\n",
        "\n",
        "# Run the RNN forward function\n",
        "ys, hs = rnn_forward(x, Wxh, Whh, Why, bh, by, h0)\n",
        "\n",
        "print(\"Outputs at each time step:\")\n",
        "for t, y in enumerate(ys):\n",
        "    print(f\"Time step {t+1}: y = {y.flatten()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0FsYFLKD-YM",
        "outputId": "e8b52c06-0c62-4646-944a-e35cad5327d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs at each time step:\n",
            "Time step 1: y = [-0.00050584]\n",
            "Time step 2: y = [-0.00101643]\n",
            "Time step 3: y = [-0.00152624]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10 : Implementation of the self-attention mechanism using only NumPy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "    \"\"\"Compute the softmax of each element along the specified axis of x.\"\"\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))  # For numerical stability\n",
        "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
        "\n",
        "def self_attention(X, Wq, Wk, Wv):\n",
        "    # Compute Queries (Q), Keys (K), and Values (V)\n",
        "    Q = np.dot(X, Wq)  # Shape: (n, dout)\n",
        "    K = np.dot(X, Wk)  # Shape: (n, dout)\n",
        "    V = np.dot(X, Wv)  # Shape: (n, dout)\n",
        "\n",
        "    d_k = Q.shape[1]\n",
        "    attention_scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
        "    attention_weights = softmax(attention_scores, axis=-1)\n",
        "    output = np.dot(attention_weights, V)\n",
        "    return output\n",
        "\n",
        "\n",
        "np.random.seed(0)  # For reproducibility\n",
        "\n",
        "# Input matrix X (n=4 vectors, d=3 features per vector)\n",
        "X = np.random.rand(4, 3)  # Shape: (4, 3)\n",
        "\n",
        "# Learnable weight matrices Wq, Wk, Wv\n",
        "d = 3      # Input dimension\n",
        "dout = 2   # Output dimension\n",
        "Wq = np.random.rand(d, dout)  # Shape: (3, 2)\n",
        "Wk = np.random.rand(d, dout)  # Shape: (3, 2)\n",
        "Wv = np.random.rand(d, dout)  # Shape: (3, 2)\n",
        "\n",
        "# Call the self_attention function\n",
        "output = self_attention(X, Wq, Wk, Wv)\n",
        "\n",
        "print(\"Input Matrix X:\")\n",
        "print(X)\n",
        "print(\"\\nWeight Matrix Wq:\")\n",
        "print(Wq)\n",
        "print(\"\\nWeight Matrix Wk:\")\n",
        "print(Wk)\n",
        "print(\"\\nWeight Matrix Wv:\")\n",
        "print(Wv)\n",
        "print(\"\\nSelf-Attention Output:\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLqJllcYD-U-",
        "outputId": "5b2aee69-c413-43cc-f4d7-43b50dfd40bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Matrix X:\n",
            "[[0.5488135  0.71518937 0.60276338]\n",
            " [0.54488318 0.4236548  0.64589411]\n",
            " [0.43758721 0.891773   0.96366276]\n",
            " [0.38344152 0.79172504 0.52889492]]\n",
            "\n",
            "Weight Matrix Wq:\n",
            "[[0.56804456 0.92559664]\n",
            " [0.07103606 0.0871293 ]\n",
            " [0.0202184  0.83261985]]\n",
            "\n",
            "Weight Matrix Wk:\n",
            "[[0.77815675 0.87001215]\n",
            " [0.97861834 0.79915856]\n",
            " [0.46147936 0.78052918]]\n",
            "\n",
            "Weight Matrix Wv:\n",
            "[[0.11827443 0.63992102]\n",
            " [0.14335329 0.94466892]\n",
            " [0.52184832 0.41466194]]\n",
            "\n",
            "Self-Attention Output:\n",
            "[[0.53569849 1.29450415]\n",
            " [0.53551973 1.29413435]\n",
            " [0.53849796 1.29925955]\n",
            " [0.53131543 1.28657939]]\n"
          ]
        }
      ]
    }
  ]
}